# Chapter 10: Sample-Efficient Value-Based Methods - á€¡á€€á€»á€‰á€ºá€¸á€á€»á€¯á€•á€º

## 1. Chapter á€›á€²á€· á€›á€Šá€ºá€›á€½á€šá€ºá€á€»á€€á€º

á€’á€® Chapter á€™á€¾á€¬ value-based DRL á€›á€²á€· **sample efficiency** á€€á€­á€¯ á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸á€¡á€±á€¬á€„á€º improvements á‚ á€á€¯ á€‘á€•á€ºá€‘á€Šá€·á€ºá€•á€«á€á€šá€º: **Dueling Architecture** á€”á€¾á€„á€·á€º **Prioritized Experience Replay (PER)**á‹

```mermaid
graph LR
    DDQN["DDQN<br/>(Ch 9)"] -->|"+ Dueling Architecture<br/>+ Polyak Averaging"| DUEL["Dueling DDQN"]
    DUEL -->|"+ Prioritized Replay<br/>+ Importance Sampling"| PER["Dueling DDQN<br/>+ PER"]
    
    style DDQN fill:#ff922b,color:#fff
    style DUEL fill:#2196F3,color:#fff
    style PER fill:#4CAF50,color:#fff
```

á€¡á€“á€­á€€ á€¡á€€á€¼á€±á€¬á€„á€ºá€¸á€¡á€›á€¬á€™á€»á€¬á€¸:
1. **Dueling network** â€” Q(s,a) á€€á€­á€¯ V(s) + A(s,a) á€á€½á€²á€•á€¼á€®á€¸ learn
2. **Polyak averaging** â€” smooth target network updates
3. **Prioritized Experience Replay** â€” TD error á€–á€¼á€„á€·á€º priority sampling
4. **Importance sampling** â€” prioritization bias á€€á€­á€¯ correct
5. **Value-based DRL á€›á€²á€· evolution** â€” NFQ â†’ DQN â†’ DDQN â†’ Dueling DDQN â†’ PER

---

## 2. Value Functions á€›á€²á€· Relationship

### Q, V, A Functions

$$Q(s, a) = V(s) + A(s, a)$$

| Function | Meaning | Output |
|---|---|---|
| **Q(s, a)** | State-action value | Action á€›á€²á€· expected return |
| **V(s)** | State value | State á€›á€²á€· expected return (avearge over actions) |
| **A(s, a)** | Advantage | Action a á€€ average á€‘á€€á€º á€˜á€šá€ºá€œá€±á€¬á€€á€º better/worse |

$$A(s, a) = Q(s, a) - V(s)$$

$$\mathbb{E}_{a \sim \pi}[A(s, a)] = 0$$

> ğŸ’¡ Advantage function á€›á€²á€· expectation á€€ **0** á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹ Policy á€›á€²á€· default action á€€á€­á€¯ á€šá€°á€›á€á€¬ (**V(s)**)á€‘á€€á€º action a á€€ á€˜á€šá€ºá€œá€±á€¬á€€á€º better/worse á€†á€­á€¯á€á€¬ á€•á€¼á€•á€«á€á€šá€ºá‹

---

## 3. Dueling Network Architecture

### Regular vs Dueling Architecture

```mermaid
graph TD
    subgraph REG["Regular Architecture"]
        RI["Input (4)"] --> RH1["Hidden 1 (512)"]
        RH1 --> RH2["Hidden 2 (128)"]
        RH2 --> RQ["Q(s,a): 2 outputs<br/>[Q(s,left), Q(s,right)]"]
    end
    
    subgraph DUEL_ARCH["Dueling Architecture âœ…"]
        DI["Input (4)"] --> DH1["Hidden 1 (512)"]
        DH1 --> DH2["Hidden 2 (128)"]
        DH2 --> DV["V(s): 1 output"]
        DH2 --> DA["A(s,a): 2 outputs"]
        DV --> AGG["Aggregate:<br/>Q = V + A - mean(A)"]
        DA --> AGG
    end
    
    style DUEL_ARCH fill:#4CAF50,color:#fff
    style AGG fill:#ffd43b,color:#000
```

### á€˜á€¬á€€á€¼á€±á€¬á€„á€·á€º Dueling Architecture á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸á€á€¬á€œá€²

**Regular network** â€” Q(s, left) á€”á€²á€· Q(s, right) á€€á€­á€¯ separately learn á€›á€á€šá€º:
- Sample á€á€…á€ºá€á€¯á€€ action á€á€…á€ºá€á€¯á€›á€²á€· Q-value á€€á€­á€¯á€á€¬ update

**Dueling network** â€” V(s) á€€á€­á€¯ share á€•á€¼á€®á€¸ A(s,a) á€›á€²á€· differences á€á€¬ learn:
- Sample á€á€…á€ºá€á€¯á€€ **V(s) á€€á€­á€¯ update** â†’ **all actions** á€›á€²á€· Q-values improve!
- **Sample efficiency** dramatically improve

```mermaid
graph TD
    subgraph OLD["âŒ Regular: Single Sample Updates"]
        OS["Experience (s, left, r, s')"] --> OQ1["Updates Q(s, left) only"]
        OQ1 -.->|"Q(s, right)<br/>unchanged"| OQ2["Q(s, right)"]
    end
    
    subgraph NEW["âœ… Dueling: Shared V(s) Updates"]
        NS["Experience (s, left, r, s')"] --> NV["Updates V(s)"]
        NV --> NQ1["Q(s, left) improves"]
        NV --> NQ2["Q(s, right) also improves!"]
    end
    
    style NEW fill:#4CAF50,color:#fff
    style OLD fill:#ef5350,color:#fff
```

### Cart-Pole Example

Pole upright position (perfect state) â†’ left/right values nearly equal:
- $Q(s, \text{left}) \approx Q(s, \text{right})$
- $V(s)$ â‰ˆ high, $A(s, \text{left}) \approx A(s, \text{right}) \approx 0$

Pole tilted right:
- $A(s, \text{right}) > 0$ (push right â†’ correct tilt)
- $A(s, \text{left}) < 0$ (push left â†’ worsen tilt)

---

## 4. FCDuelingQ: PyTorch Implementation

### Building the Dueling Network

```python
class FCDuelingQ(nn.Module):
    def __init__(self, input_dim, output_dim,
                 hidden_dims=(32, 32), activation_fc=F.relu):
        super(FCDuelingQ, self).__init__()
        self.activation_fc = activation_fc
        
        # Shared layers
        self.input_layer = nn.Linear(input_dim, hidden_dims[0])
        self.hidden_layers = nn.ModuleList()
        for i in range(len(hidden_dims) - 1):
            self.hidden_layers.append(
                nn.Linear(hidden_dims[i], hidden_dims[i+1]))
        
        # Two separate output streams
        self.value_output = nn.Linear(hidden_dims[-1], 1)       # V(s): single value
        self.advantage_output = nn.Linear(hidden_dims[-1], output_dim)  # A(s,a): per action
```

### Aggregating Equation (Forward Pass)

$$Q(s, a; \theta, \alpha, \beta) = V(s; \theta, \beta) + \left( A(s, a; \theta, \alpha) - \frac{1}{|\mathcal{A}|}\sum_{a'} A(s, a'; \theta, \alpha) \right)$$

- $\theta$ â€” shared layer weights
- $\alpha$ â€” advantage stream weights
- $\beta$ â€” value stream weights

```python
def forward(self, state):
    x = state
    if not isinstance(x, torch.Tensor):
        x = torch.tensor(x, device=self.device, dtype=torch.float32)
        x = x.unsqueeze(0)
    
    x = self.activation_fc(self.input_layer(x))
    for hidden_layer in self.hidden_layers:
        x = self.activation_fc(hidden_layer(x))
    
    a = self.advantage_output(x)          # A(s, a)
    v = self.value_output(x).expand_as(a) # V(s) â†’ expand to match A shape
    
    # Aggregation: Q = V + (A - mean(A))
    q = v + a - a.mean(1, keepdim=True).expand_as(a)
    return q
```

> âš ï¸ V(s) á€”á€²á€· A(s,a) á€€á€­á€¯ uniquely recover á€™á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€•á€« (V+10 á€”á€²á€· A-10 á€†á€­á€¯á€›á€„á€º Q á€¡á€á€°á€á€° á€›á€”á€­á€¯á€„á€º)á‹ **mean(A) á€€á€­á€¯ subtract** á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸á€–á€¼á€„á€·á€º one degree of freedom á€€á€­á€¯ á€–á€šá€ºá€•á€¼á€®á€¸ optimization á€…á€­á€á€ºá€á€»á€› stabilize á€–á€¼á€…á€ºá€¡á€±á€¬á€„á€º á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹

---

## 5. Polyak Averaging â€” Smooth Target Updates

### Full Update vs Polyak Averaging

```mermaid
graph TD
    subgraph FULL["âŒ Full Update (old way)"]
        F1["Freeze target for N steps"]
        F2["Progressively stale data"]
        F3["Big sudden update"]
    end
    
    subgraph POLY["âœ… Polyak Averaging (new)"]
        P1["Mix online into target<br/>every step"]
        P2["Always slightly lagging"]
        P3["Smooth, stable updates"]
    end
    
    style FULL fill:#ff922b,color:#fff
    style POLY fill:#4CAF50,color:#fff
```

### Formula

$$\theta^- \leftarrow (1 - \tau) \cdot \theta^- + \tau \cdot \theta$$

- $\tau = 0.1$ â†’ target network = 90% old + 10% new (every step)
- $\tau = 1.0$ â†’ full copy (equivalent to old method)

### Implementation

```python
def update_network(self, tau=None):
    tau = self.tau if tau is None else tau
    for target, online in zip(
        self.target_model.parameters(),
        self.online_model.parameters()):
        target_ratio = (1.0 - tau) * target.data    # 90% target
        online_ratio = tau * online.data              # 10% online
        mixed_weights = target_ratio + online_ratio
        target.data.copy_(mixed_weights)
```

| Update Method | Pros | Cons |
|---|---|---|
| **Full update** (every N steps) | Simple | Stale data between updates, sudden big change |
| **Polyak averaging** (every step) | Smooth, always recent | Need to tune Ï„ |

---

## 6. Prioritized Experience Replay (PER)

### Uniform vs Prioritized Sampling

```mermaid
graph TD
    subgraph UNI["Uniform Sampling (DQN/DDQN)"]
        U1["All experiences = equal probability"]
        U2["Unbiased but wasteful"]
        U3["Time spent on<br/>uninformative experiences"]
    end
    
    subgraph PRI["Prioritized Sampling (PER) âœ…"]
        PR1["High |TD error| = high priority"]
        PR2["Surprising experiences replayed more"]
        PR3["More efficient learning"]
    end
    
    style PRI fill:#4CAF50,color:#fff
    style UNI fill:#ff922b,color:#fff
```

### TD Error as Priority

$$\text{priority}_i = |\delta_i| = |Q(s_i, a_i; \theta) - y_i|$$

> ğŸ’¡ High reward á€Ÿá€¯á€á€ºá€›á€„á€ºá€á€¬ replay á€œá€¯á€•á€ºá€–á€­á€¯á€· á€€á€¼á€­á€¯á€¸á€…á€¬á€¸á€á€¬ **á€™á€¾á€¬á€¸á€•á€«**á€á€šá€º! Agent á€€á€­á€¯ high/low/mundane experiences **á€¡á€¬á€¸á€œá€¯á€¶á€¸** á€€á€”á€± learn á€á€­á€¯á€„á€ºá€¸á€›á€™á€šá€ºá‹ **Surprise (TD error)** á€€á€á€¬ learning opportunity á€›á€²á€· best proxy á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

### Greedy Prioritization á€›á€²á€· Problems

1. TD error = 0 á€–á€¼á€…á€ºá€›á€„á€º â†’ replay á€‘á€•á€ºá€™á€–á€¼á€…á€ºá€”á€­á€¯á€„á€º
2. Function approximators á€€á€¼á€±á€¬á€„á€·á€º errors á€”á€¾á€±á€¸á€”á€¾á€±á€¸ shrink â†’ small subset á€•á€±á€«á€ºá€™á€¾á€¬ fixate
3. TD errors are noisy â†’ noise follow á€–á€¼á€…á€ºá€”á€­á€¯á€„á€º

**á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€á€»á€€á€º**: Stochastic prioritization (greedy á€™á€Ÿá€¯á€á€ºá€˜á€² probability-based sampling)

---

## 7. Proportional vs Rank-based Prioritization

### Proportional Prioritization

$$p_i = |\delta_i| + \epsilon$$

- $\epsilon$ â€” small constant (zero TD error á€–á€¼á€…á€ºá€›á€„á€ºá€œá€Šá€ºá€¸ replay á€–á€¼á€…á€ºá€–á€­á€¯á€·)

### Rank-based Prioritization

$$p_i = \frac{1}{\text{rank}(i)}$$

- rank = position when sorted by |TD error| (descending)
- Outlier-resistant (proportional á€‘á€€á€º robust)

### Priorities â†’ Probabilities

$$P(i) = \frac{p_i^\alpha}{\sum_k p_k^\alpha}$$

- $\alpha = 0$: **Uniform** sampling (all equal)
- $\alpha = 1$: **Full prioritization** (proportional to TD error)
- $0 < \alpha < 1$: **Blend** between uniform and prioritized

```mermaid
graph LR
    TD["|TD error|"] -->|"+ Îµ"| PRI["Priority p_i"]
    PRI -->|"^Î±, normalize"| PROB["Probability P(i)"]
    PROB -->|"sample"| BATCH["Mini-batch"]
    
    style TD fill:#ef5350,color:#fff
    style PRI fill:#ffd43b,color:#000
    style PROB fill:#2196F3,color:#fff
    style BATCH fill:#4CAF50,color:#fff
```

---

## 8. Importance Sampling â€” Bias Correction

### á€˜á€¬á€€á€¼á€±á€¬á€„á€·á€º Correction á€œá€­á€¯á€á€œá€²

Prioritized sampling á€Ÿá€¬ data distribution á€€á€­á€¯ á€•á€¼á€±á€¬á€„á€ºá€¸á€œá€­á€¯á€€á€ºá€•á€«á€á€šá€º:
- High-priority experiences: over-sampled
- Low-priority experiences: under-sampled
- **Biased estimates** á€–á€¼á€…á€ºá€”á€­á€¯á€„á€º (convergence á€€á€­á€¯ harm)

### Importance Sampling Weights

$$w_i = \left(\frac{1}{N \cdot P(i)}\right)^\beta$$

$$\hat{w}_i = \frac{w_i}{\max_j w_j}$$

- $\beta = 0$: **No correction** (ignoring bias)
- $\beta = 1$: **Full correction** (completely unbiased)
- $\beta$ á€€á€­á€¯ 0.1 á€€á€”á€± 1.0 á€†á€® slowly anneal

> ğŸ’¡ Max weight á€–á€¼á€„á€·á€º normalize á€á€¼á€„á€ºá€¸á€€á€¼á€±á€¬á€„á€·á€º **largest weight = 1** á€–á€¼á€…á€ºá€•á€¼á€®á€¸ TD errors á€€á€­á€¯ downscale only (upscale á€™á€–á€¼á€…á€º) â†’ training stability maintain

### Loss Function with IS Weights

$$\mathcal{L}(\theta) = \frac{1}{N}\sum_{i=1}^{N} \hat{w}_i \cdot (Q(s_i, a_i; \theta) - y_i)^2$$

```python
# PER loss function
td_error = q_sa - target_q_sa
value_loss = (weights * td_error).pow(2).mul(0.5).mean()  # weights = IS weights

# Update priorities after training
priorities = np.abs(td_error.detach().cpu().numpy())
self.replay_buffer.update(idxs, priorities)
```

---

## 9. PER Buffer Implementation

```python
class PrioritizedReplayBuffer():
    def store(self, sample):
        # New experience â†’ max priority (ensure replayed at least once)
        priority = 1.0
        if self.n_entries > 0:
            priority = self.memory[:self.n_entries, self.td_error_index].max()
        self.memory[self.next_index, self.td_error_index] = priority
        self.memory[self.next_index, self.sample_index] = np.array(sample)
        self.n_entries = min(self.n_entries + 1, self.max_samples)
        self.next_index = (self.next_index + 1) % self.max_samples
    
    def update(self, idxs, td_errors):
        # Update TD errors for replayed experiences
        self.memory[idxs, self.td_error_index] = np.abs(td_errors)
    
    def sample(self, batch_size=None):
        # Calculate priorities
        if self.rank_based:
            priorities = 1 / (np.arange(self.n_entries) + 1)
        else:  # proportional
            priorities = entries[:, self.td_error_index] + EPS
        
        # Priorities â†’ Probabilities
        scaled_priorities = priorities ** self.alpha
        probs = scaled_priorities / np.sum(scaled_priorities)
        
        # Importance sampling weights
        weights = (self.n_entries * probs) ** (-self.beta)
        normalized_weights = weights / weights.max()
        
        # Sample using probabilities
        idxs = np.random.choice(self.n_entries, batch_size,
                                replace=False, p=probs)
        return idxs, normalized_weights[idxs], samples
```

---

## 10. Full Dueling DDQN + PER Algorithm

### Architecture & Hyperparameters

```mermaid
graph TD
    subgraph AGENT["Dueling DDQN + PER Agent"]
        ONLINE["Online Dueling Network<br/>Q(s,a; Î¸,Î±,Î²)"]
        TARGET["Target Dueling Network<br/>Polyak avg (Ï„=0.1)"]
        PER_BUF["PER Buffer<br/>(10,000, Î±=0.6, Î² annealing)"]
        STRAT["Exp-Decay Îµ-greedy"]
    end
    
    ENV["Environment"] -->|"(s,a,r,s',d)"| PER_BUF
    PER_BUF -->|"prioritized sample<br/>+ IS weights"| TRAIN["Double Learning<br/>Online selects, Target evaluates"]
    TRAIN --> LOSS["IS-weighted MSE + RMSprop"]
    LOSS -->|"update Î¸"| ONLINE
    LOSS -->|"update priorities"| PER_BUF
    ONLINE -->|"Polyak avg<br/>every step"| TARGET
    STRAT --> ONLINE
    ONLINE -->|"action"| ENV
    
    style ONLINE fill:#2196F3,color:#fff
    style TARGET fill:#9C27B0,color:#fff
    style PER_BUF fill:#ffd43b,color:#000
```

### Hyperparameters Summary

| Parameter | Value |
|---|---|
| Architecture | Dueling (4, 512, 128, [1; 2], 2) |
| Objective | Approximate $q^*(s,a)$ |
| Learning rate | 0.0007 (RMSprop) |
| Target update | Polyak averaging (Ï„=0.1, every step) |
| Exploration | Exp-decay Îµ-greedy (1.0 â†’ 0.3) |
| Loss | IS-weighted MSE (grad clip = âˆ) |
| Double learning | âœ… |
| Buffer capacity | 10,000 |
| Batch size | 64 |
| PER Î± | 0.6 (degree of prioritization) |
| PER Î² | 0.1 â†’ 1.0 (anneal rate 0.99992, ~30k steps) |

### Algorithm Steps

1. **Collect** experience â†’ insert with **max priority** into PER buffer
2. **Sample** mini-batch using **priority probabilities** + **IS weights**
3. **Calculate** TD targets using **double learning** (online selects, target evaluates)
4. **Fit** Q-network with **IS-weighted MSE** + RMSprop
5. **Update** priorities of replayed experiences with new |TD errors|
6. **Polyak update** target network every step

---

## 11. Value-Based DRL Evolution Summary

```mermaid
graph TD
    NFQ["NFQ<br/>Ch 8: Batch + Fitting<br/>~2500 episodes"] --> DQN["DQN<br/>Ch 9: + Target Net + Replay<br/>~250 episodes (10Ã— better)"]
    DQN --> DDQN["DDQN<br/>Ch 9: + Double Learning<br/>More stable, consistent"]
    DDQN --> DUEL["Dueling DDQN<br/>Ch 10: + Dueling Architecture<br/>+ Polyak Averaging"]
    DUEL --> PER_F["Dueling DDQN + PER<br/>Ch 10: + Prioritized Replay<br/>+ IS Correction<br/>Most sample-efficient!"]
    
    style NFQ fill:#ff922b,color:#fff
    style DQN fill:#64B5F6,color:#fff
    style DDQN fill:#2196F3,color:#fff
    style DUEL fill:#1565C0,color:#fff
    style PER_F fill:#4CAF50,color:#fff
```

### Method Comparison

| Method | Key Addition | Sample Efficiency | Stability |
|---|---|---|---|
| **NFQ** | Batch fitting | â­ | â­ |
| **DQN** | Target net + Replay | â­â­â­ | â­â­ |
| **DDQN** | Double learning | â­â­â­ | â­â­â­ |
| **Dueling DDQN** | V+A architecture + Polyak | â­â­â­â­ | â­â­â­â­ |
| **+ PER** | Priority sampling + IS | â­â­â­â­â­ | â­â­â­â­ |

### Value-Based DRL á€›á€²á€· Known Issues ("Deadly Triad")

```mermaid
graph TD
    DT["âš ï¸ Deadly Triad"] --> B["Bootstrapping<br/>(TD targets)"]
    DT --> O["Off-policy Learning<br/>(Q-learning/DQN)"]
    DT --> F["Function Approximation<br/>(Neural Networks)"]
    
    B --> DIV["Potential<br/>Divergence"]
    O --> DIV
    F --> DIV
    
    style DT fill:#ef5350,color:#fff
    style DIV fill:#ff922b,color:#fff
```

**Practical advice:**
- Target networks + replay buffers + double learning á€á€¯á€¶á€¸
- Sufficiently small learning rates (but not too small)
- Patience ğŸ™‚

---

## 12. Key Equations Summary

| Equation | Formula |
|---|---|
| **Q decomposition** | $Q(s,a) = V(s) + A(s,a)$ |
| **Dueling aggregation** | $Q = V + A - \frac{1}{|\mathcal{A}|}\sum_{a'} A(s,a')$ |
| **Polyak averaging** | $\theta^- \leftarrow (1-\tau)\theta^- + \tau\theta$ |
| **Proportional priority** | $p_i = |\delta_i| + \epsilon$ |
| **Rank-based priority** | $p_i = \frac{1}{\text{rank}(i)}$ |
| **Priority â†’ Probability** | $P(i) = \frac{p_i^\alpha}{\sum_k p_k^\alpha}$ |
| **IS weights** | $w_i = (N \cdot P(i))^{-\beta}$ |
| **IS-weighted loss** | $\mathcal{L} = \frac{1}{N}\sum_i \hat{w}_i (Q(s_i,a_i;\theta) - y_i)^2$ |

---

## 13. á€”á€­á€‚á€¯á€¶á€¸á€á€»á€¯á€•á€º (Conclusion)

á€’á€® Chapter á€™á€¾á€¬ á€á€„á€ºá€šá€°á€á€²á€·á€á€²á€· á€¡á€“á€­á€€ takeaways:

1. **Dueling architecture** â€” Q(s,a) á€€á€­á€¯ V(s) + A(s,a) á€á€½á€²á€•á€¼á€®á€¸ learn á€á€¼á€„á€ºá€¸á€–á€¼á€„á€·á€º sample efficiency á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸
2. **V(s) sharing** â€” single experience tuple á€€ all actions á€›á€²á€· values á€€á€­á€¯ improve
3. **Aggregation trick** â€” mean(A) subtract á€–á€¼á€„á€·á€º V, A uniquely recover á€›á€¡á€±á€¬á€„á€º
4. **Polyak averaging** â€” full copy update á€¡á€…á€¬á€¸ smooth mixing (Ï„=0.1) every step
5. **PER** â€” TD error-based priority á€–á€¼á€„á€·á€º informative experiences á€€á€­á€¯ á€•á€­á€¯á€™á€­á€¯ replay
6. **Proportional vs Rank-based** â€” two strategies for calculating priorities
7. **Importance sampling** â€” prioritized sampling á€›á€²á€· bias á€€á€­á€¯ Î²-annealing á€–á€¼á€„á€·á€º correct
8. **Deadly triad** â€” bootstrapping + off-policy + function approximation = divergence risk

> ğŸ’¡ Chapters 8-10 á€á€Šá€º **value-based DRL** á€›á€²á€· complete survey á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹ NFQ â†’ DQN â†’ DDQN â†’ Dueling DDQN â†’ PER á€Ÿá€¯ evolution á€•á€¼á€¯á€œá€¯á€•á€ºá€á€²á€·á€•á€¼á€®á€¸ á€’á€® improvements á€¡á€¬á€¸á€œá€¯á€¶á€¸á€á€Šá€º **Lego blocks** á€€á€²á€·á€á€­á€¯á€· plug-and-play á€•á€¼á€¯á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹
